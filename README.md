# Sistema de ClasificaciÃ³n Supervisada - Tier 1

Sistema modular de clasificaciÃ³n jerÃ¡rquica de artÃ­culos usando BERT en espaÃ±ol, diseÃ±ado para ejecutarse en Google Colab con capacidad de reentrenamiento incremental.

## ğŸ“‹ DescripciÃ³n

Este sistema clasifica artÃ­culos en dos niveles jerÃ¡rquicos:
- **Nivel 1**: CategorÃ­a
- **Nivel 2**: Familia

Utiliza BERT pre-entrenado en espaÃ±ol (`dccuchile/bert-base-spanish-wwm-uncased`) con restricciones jerÃ¡rquicas para garantizar predicciones consistentes.

## ğŸ—ï¸ Estructura del Proyecto

```
clasificacion_supervisada_tier_1/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py              # ConfiguraciÃ³n central (paths, parÃ¡metros)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py       # Limpieza y preparaciÃ³n de datos
â”‚   â”œâ”€â”€ encoders.py           # CodificaciÃ³n de labels y jerarquÃ­as
â”‚   â”œâ”€â”€ model.py              # Arquitectura del modelo BERT
â”‚   â”œâ”€â”€ training.py           # LÃ³gica de entrenamiento
â”‚   â”œâ”€â”€ inference.py          # Predicciones y filtrado
â”‚   â””â”€â”€ utils.py              # Utilidades (versionado, I/O)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_train_model.ipynb         # Entrenamiento inicial
â”‚   â”œâ”€â”€ 02_retrain_model.ipynb       # Reentrenamiento incremental
â”‚   â””â”€â”€ 03_inference_pipeline.ipynb  # Pipeline de producciÃ³n
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Datos crudos
â”‚   â”œâ”€â”€ processed/            # Datos procesados
â”‚   â””â”€â”€ dictionaries/         # Diccionarios y artifacts
â”œâ”€â”€ models/                   # Modelos entrenados (.pth)
â”œâ”€â”€ outputs/                  # Resultados CSV con timestamp
â””â”€â”€ README.md
```

## ğŸš€ Flujo de Trabajo

### 1. Entrenamiento Inicial (`01_train_model.ipynb`)
1. Carga datos de entrenamiento (`train_data_v1.csv`)
2. Preprocesa y limpia texto (corpus)
3. Codifica labels jerÃ¡rquicos
4. Entrena modelo BERT desde cero
5. Guarda:
   - `model_base.pth`
   - `codificador_base.pkl`
   - `class_weights_base.pth`
   - `hierarchy_map_base.json`

### 2. Reentrenamiento (`02_retrain_model.ipynb`)
1. Carga nueva data validada (`data_to_retrain.csv`)
2. Carga modelo y codificador existente
3. Reentrena con nueva data
4. Guarda versiÃ³n incremental:
   - `model_retrain_v1.pth`, `model_retrain_v2.pth`, etc.
   - Artifacts correspondientes versionados

### 3. Inferencia en ProducciÃ³n (`03_inference_pipeline.ipynb`)
1. Carga datos mensuales (`datos_importacion_mensual.csv`)
2. Preprocesa con diccionario de aranceles
3. Realiza predicciones con restricciones jerÃ¡rquicas
4. Aplica threshold de confianza (86%)
5. Genera outputs versionados:
   - `clasificacion_tier_1_YYYYMMDD_HHMMSS.csv` (â‰¥ 86% confianza)
   - `articulos_no_clasificados_YYYYMMDD_HHMMSS.csv` (< 86% confianza)

## ğŸ”§ ConfiguraciÃ³n para Colab

### Primera Celda en cada Notebook:

```python
import sys
IS_COLAB = 'google.colab' in sys.modules

if IS_COLAB:
    from google.colab import drive
    from config.config import Config
    
    config = Config(is_colab=True)
    config.setup_colab_environment()
else:
    from config.config import Config
    config = Config(is_colab=False)
```

### Estructura en Google Drive:

```
/content/drive/MyDrive/
â””â”€â”€ clasificacion_2_niveles/
    â””â”€â”€ laboratory_2/
        â”œâ”€â”€ config/
        â”œâ”€â”€ src/
        â”œâ”€â”€ notebooks/
        â”œâ”€â”€ data/
        â”œâ”€â”€ models/
        â””â”€â”€ outputs/
```

## ğŸ“¦ Dependencias

```python
# Colab ya incluye la mayorÃ­a, pero verifica:
torch>=1.10.0
transformers>=4.20.0
pandas>=1.3.0
scikit-learn>=1.0.0
nltk>=3.6
tqdm>=4.62.0
```

## ğŸ¯ ParÃ¡metros Clave

Modificables en `config/config.py`:

```python
bert_model_name = "dccuchile/bert-base-spanish-wwm-uncased"
max_seq_length = 512
batch_size = 16
learning_rate = 2e-5
num_epochs = 3
classification_threshold = 0.86  # Threshold de confianza
```

## ğŸ“Š Versionado de Outputs

Todos los CSV de salida incluyen timestamp automÃ¡tico:
- `clasificacion_tier_1_20231215_143025.csv`
- `articulos_no_clasificados_20231215_143025.csv`

Los modelos reentrenados usan versionado incremental:
- `model_retrain_v1.pth`, `model_retrain_v2.pth`, etc.

## ğŸ”„ Ciclo de Mejora Continua

```
Datos Mensuales
    â†“
Inferencia (03_inference_pipeline.ipynb)
    â†“
Filtrado por Threshold 86%
    â†“
â”œâ”€â†’ Clasificados â†’ ProducciÃ³n (clasificacion_tier_1.csv)
â””â”€â†’ No Clasificados â†’ AnÃ¡lisis Manual + Sugerencias AI
                      â†“
                Data Validada
                      â†“
            Reentrenamiento (02_retrain_model.ipynb)
                      â†“
                Nueva VersiÃ³n del Modelo
```

## ğŸ“ Archivos de Entrada Requeridos

### data/raw/
- `train_data_v1.csv`: Datos iniciales de entrenamiento
  - Columnas: `descripcion`, `cod_arancelario`, `nombre_categoria`, `familia`
- `dicaranceles.xlsx`: Diccionario de cÃ³digos arancelarios
  - Columnas: `Codigo`, `Descripcion`
- `datos_importacion_mensual.csv`: Datos para clasificar
- `data_to_retrain.csv`: Nueva data validada para reentrenar

### data/dictionaries/
- `stopwords_spanish.json`: Lista de stopwords
- `replacement_dict.json`: Diccionario de reemplazos de texto

## ğŸ§  CaracterÃ­sticas del Modelo

### Arquitectura JerÃ¡rquica
- **Classifier Categoria**: BERT â†’ Linear(768, N_categorias)
- **Classifier Familia**: BERT + Categoria â†’ Linear(768+N_cat, N_familias)
- **Constraint Enforcement**: Masking de familias invÃ¡lidas durante predicciÃ³n

### Training
- Loss: CrossEntropy ponderado + PenalizaciÃ³n jerÃ¡rquica
- Optimizer: AdamW
- Dropout: 0.3 en capas de clasificaciÃ³n
- Class Weights: Balanceo automÃ¡tico por frecuencia

### Inference
- PredicciÃ³n con restricciones jerÃ¡rquicas
- Score de confianza: promedio de probabilidades
- Threshold configurable (default: 86%)

## ğŸ“– Uso RÃ¡pido

### En Colab:

1. Monta Drive y carga el proyecto
2. Abre el notebook deseado
3. Ejecuta todas las celdas

### Localmente (desarrollo):

```python
from config.config import Config
config = Config(is_colab=False)

# Importar mÃ³dulos
from src import get_data, preprocess_data, train_model
```

## âš ï¸ Notas Importantes

- **Corpus**: ConcatenaciÃ³n de descripciÃ³n + descripciÃ³n de arancel
- **TokenizaciÃ³n**: NLTK word_tokenize
- **Limpieza**: Lowercase, sin puntuaciÃ³n, sin nÃºmeros, filtrado de stopwords
- **GPU**: Detecta automÃ¡ticamente CUDA si estÃ¡ disponible
- **Memory**: Batch size ajustable segÃºn GPU disponible

## ğŸ› Troubleshooting

### Error: Codificador no encuentra clases nuevas
- **SoluciÃ³n**: Usar el codificador base siempre para reentrenar

### Error: CUDA out of memory
- **SoluciÃ³n**: Reducir `batch_size` en config.py

### CSV sin datos clasificados
- **SoluciÃ³n**: Revisar threshold (quizÃ¡s muy alto), verificar modelo cargado

## ğŸ“„ Licencia

Proyecto interno - Todos los derechos reservados

## ğŸ‘¥ Contacto

Para dudas sobre el sistema, consultar con el equipo de Data Science.
